<!DOCTYPE html>
<html lang="en" dir="auto">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" integrity="sha384-........" crossorigin="anonymous"></script>
<script>
MathJax = {
    tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true
    },
    svg: {
    fontCache: 'global'
    }
};
</script>


<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Multimodal prompting for image generation | Nihal Jain</title>
<meta name="keywords" content="multimodal-ml, image-generation, diffusion-models">
<meta name="description" content="Enabling the use of multiple modalities while prompting Stable Diffusion.">
<meta name="author" content="Nihal Jain">
<link rel="canonical" href="https://nihaljn.github.io/blog/multimodal-prompting/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.2f9a6f61cc4f7914f7e2a5b81d36553a7ef3561e4d81db12baa01d90271436f7.css" integrity="sha256-L5pvYcxPeRT34qW4HTZVOn7zVh5NgdsSuqAdkCcUNvc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://nihaljn.github.io/columbia-favicon.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://nihaljn.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://nihaljn.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://nihaljn.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://nihaljn.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://nihaljn.github.io/blog/multimodal-prompting/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>




<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@200..900&display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+Devanagari:wght@100..900&display=swap" rel="stylesheet"><link href="https://cdn.jsdelivr.net/gh/bitmaks/cm-web-fonts@latest/fonts.css" rel="stylesheet">

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script><script>
document.addEventListener('DOMContentLoaded', function() {
    const toggleButton = document.getElementById('theme-toggle');
    if (toggleButton) {
        
        const moonIcon = toggleButton.querySelector('#moon');
        const sunIcon = toggleButton.querySelector('#sun');
        
        
        const wrapper = document.createElement('div');
        wrapper.className = 'theme-toggle-wrapper';
        
        
        const menu = document.getElementById('menu');
        if (menu) {
            const menuItem = document.createElement('li');
            menuItem.appendChild(wrapper);
            menu.appendChild(menuItem);
        } else {
            
            toggleButton.parentNode.insertBefore(wrapper, toggleButton);
        }
        
        if (sunIcon) {
            toggleButton.removeChild(sunIcon);
            wrapper.appendChild(sunIcon);
        }
        
        
        wrapper.appendChild(toggleButton);

        
        if (moonIcon) {
            toggleButton.removeChild(moonIcon);
            wrapper.appendChild(moonIcon);
        }
        
        
        const thumb = document.createElement('div');
        thumb.className = 'toggle-thumb';
        toggleButton.appendChild(thumb);
    }
});
</script><script>
function toggleSection(sectionId) {
    const content = document.getElementById(sectionId + '-content');
    const button = document.querySelector('.' + sectionId + '-toggle');
    const arrow = document.getElementById(sectionId + '-arrow');
    
    if (content.classList.contains('expanded')) {
        content.classList.remove('expanded');
        button.classList.remove('expanded');
        arrow.textContent = '▶';
    } else {
        content.classList.add('expanded');
        button.classList.add('expanded');
        arrow.textContent = '▼';
    }
}


function toggleMoreInfo() {
    toggleSection('more-info');
}
</script>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-9BZ254GT1K"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-9BZ254GT1K');
        }
      </script><meta property="og:url" content="https://nihaljn.github.io/blog/multimodal-prompting/">
  <meta property="og:site_name" content="Nihal Jain">
  <meta property="og:title" content="Multimodal prompting for image generation">
  <meta property="og:description" content="Enabling the use of multiple modalities while prompting Stable Diffusion.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2022-10-01T20:56:16-04:00">
    <meta property="article:modified_time" content="2022-10-01T20:56:16-04:00">
    <meta property="article:tag" content="multimodal-ml">
    <meta property="article:tag" content="image-generation">
    <meta property="article:tag" content="diffusion-models">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multimodal prompting for image generation">
<meta name="twitter:description" content="Enabling the use of multiple modalities while prompting Stable Diffusion.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blog",
      "item": "https://nihaljn.github.io/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Multimodal prompting for image generation",
      "item": "https://nihaljn.github.io/blog/multimodal-prompting/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Multimodal prompting for image generation",
  "name": "Multimodal prompting for image generation",
  "description": "Enabling the use of multiple modalities while prompting Stable Diffusion.",
  "keywords": [
    "multimodal-ml", "image-generation", "diffusion-models"
  ],
  "articleBody": "Note: Supporting code and data are available here.\nFigure 1. An illustration of Multimodal Prompting: an arbitrary composition of images and text can be used as a prompt to the Stable Diffusion model using the method presented in this article.\nIn the past few months, we have experienced groundbreaking progress on the front of image generation models. While some of these models have been available behind black-box user interfaces (or only for fixed prompts) ( Citation: Yu, Xu et al., 2022 Yu, J., Xu, Y., Koh, J., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J. \u0026 Wu, Y. (2022). Scaling autoregressive models for content-rich text-to-image generation. arXiv. https://arxiv.org/abs/2206.10789. ; Citation: Ramesh, Dhariwal et al., 2022 Ramesh, A., Dhariwal, P., Nichol, A., Chu, C. \u0026 Chen, M. (2022). Hierarchical text-conditional image generation with CLIP latents. arXiv. https://arxiv.org/abs/2204.06125. ; Citation: Gafni, Polyak et al., 2022 Gafni, O., Polyak, A., Ashual, O., Sheynin, S., Parikh, D. \u0026 Taigman, Y. (2022). Make-a-scene: Scene-based text-to-image generation with human priors. arXiv. https://arxiv.org/abs/2203.13131. ) , some of them ( Citation: Rombach, Blattmann et al., 2021 Rombach, R., Blattmann, A., Lorenz, D., Esser, P. \u0026 Ommer, B. (2021). High-resolution image synthesis with latent diffusion models. arXiv. https://arxiv.org/abs/2112.10752. ) have been open-sourced with practically no restrictions! This has led to a tremendous surge in the use, analysis and extension of models like Stable Diffusion which is now being used for very creative applications such as infinite image outpainting, textual inversion, and so on. ( Citation: Gal, Alaluf et al., 2022 Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A., Chechik, G. \u0026 Cohen-Or, D. (2022). An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv. https://arxiv.org/abs/2208.01618. ) .\nThis article presents yet another extension to the Stable Diffusion model for image generation: a simple and fast strategy towards prompting Stable Diffusion with an arbitrary composition of image and text tokens. The use of multimodal prompts can be justified for the following reasons:\nAchieving high quality, desirable outputs from image generation models requires rigorous prompt tuning. While the community has reverse-engineered several interesting tricks for working with image generation models, this leads to a bad user-experience. In zero-shot settings, when either the user or the model comes across a concept that is not within their vocabulary or too cumbersome to describe, the model may be better suited with processing the prompt if it can be “shown” the new concept and instructed to manipulate the new concept in textual format. Figure 1 shows an example of our method in action and more examples are shown below. In the following sections, we will look closer into how this can be achieved using a simple technique.\nMethod Enabling multimodal prompting requires the underlying model to have a joint understanding of the different modalities. The straightforward solution for this would be to train or finetune Stable Diffusion so that its conditioning model can input multiple modalities, so that its outputs can be continued to use as before. Another method for this could be to use textual inversion ( Citation: Gal, Alaluf et al., 2022 Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A., Chechik, G. \u0026 Cohen-Or, D. (2022). An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv. https://arxiv.org/abs/2208.01618. ) : we can finetune the model to represent each of the non-text concepts in the prompt as textual tokens, and use the newly learned tokens to replace the images in the prompts. However, in this article, we will look at a simpler strategy that requires no training or finetuning in any way, with negligible additional overhead in comparison to the original Stable Diffusion.\nThe high level overview of our method is as follows:\nMaintain a rich corpus of prompts (or sub-prompts) that can textually explain various aspects of the images in any prompt in detail;\nFor each image in the prompt, find the nearest neighboring text prompts in the above dataset by projecting the image and the above curated text prompts into the pooler representation of a text-image alignment model, such as CLIP ( Citation: Radford, Kim et al., 2021 Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G. \u0026 Sutskever, I. (2021). Learning transferable visual models from natural language supervision. arXiv. https://arxiv.org/abs/2103.00020. ) ;\nA meaningful prompt can be obtained by concatenating a diverse set of nearest neighbors obtained: this is achieved by using a popular reranking algorithm, Maximum Marginal Relevance (MMR) ( Citation: Carbonell \u0026 Goldstein, 1998 Carbonell, J. \u0026 Goldstein, J. (1998). The use of MMR, diversity-based reranking for reordering documents and producing summaries. Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval. ) over the nearest neighbors found above;\nReplace each image in the prompt with its “translated” text to obtain a text-only prompt which can be used to prompt Stable Diffusion without any finetuning or training.\nLet us look at each piece of this method in more detail.\nData: Curating an index of text prompts Our method requires being able to retrieve text prompts that can adequately describe any image used in a prompt. Towards this, we use a processed version of Simulacra Aesthetic Captions ( Citation: Pressman, Crowson et al., 2022 Pressman, J., Crowson, K. \u0026 Contributors, S. (2022). Simulacra aesthetic captions (Version 1.0). Stability AI. https://github.com/JD-P/simulacra-aesthetic-captions. ) which is a curated dataset of over 40K prompts written by users of image-generation models.\nMost prompts in this datset are long-winded and highly detailed such that they only describe their respective generated images. Thus, this set of prompts may not be able to describe a new image that has details different from any image that this dataset describes. So, we chunk each prompt in the dataset into smaller $n$-grams (in our experiments, we fix $n = 4$) such that each $n$-gram can be expected to describe only an atomic detail of some image. Thus, in expectation, combining different $n$-grams across the dataset should expand the scope of images describable by the dataset. While blind composition of random $n$-grams from the dataset may not lead to prompts that are intelligible or even grammarly correct, we can expect our retrieval strategy to obtain a diverse set of terms in the composition such that each provides meaningful signal to Stable Diffusion about what has to be generated.\nFor example, the prompt 'A crowded subway car, the fluorescent lights flickering and casting an eerie glow on the people inside' will be mapped to a list of $4$-grams as: ['A crowded subway car', 'crowded subway car, the', 'subway car, the fluorescent', ..., 'on the people inside'].\nMapping images to text Given any image, we are interested in retrieving a set of $n$-grams such that composing them adequately describes the image. An intuitive way for doing this is to find the nearest neighbors texts to a query image in the representation space of a text-image alignment model. So, we utilize CLIP’s pooled representation space to perform cross-modal nearest-neighbor search where the query is the image to be descibed and the index of candidates comprises all the $n$-grams curated above.\nSimply picking the top-$k$ neighbors from a nearest-neighbor search described above might lead to uninteresting results. For example, if the top-$k$ text neighbors describe the same aspect of an image, the resulting combination of them will not sufficiently describe the query.\nTowards mitigating this, we use a popular re-ranking algorithm, called Maximum Marginal Relevance (MMR) to pick a diverse set of $n$-grams from the top-$k$ neighbors, which when concatenated together, describe the image more meaningfully. MMR iteratively selects documents from the top-$k$ neighbors, such that each new selected document introduces new information while still staying relevant to the query. Specifically, given the collection of top-$k$ documents, $\\mathcal{R}$, a set of already selected documents, $\\mathcal{S}$, and a query $\\mathcal{Q}$, MMR augments the set of selected documents as:\n$$\\mathcal{S} \\leftarrow \\mathcal{S}\\ \\ \\cup\\ \\ \\left\\lbrace\\argmax_{D_i \\in \\mathcal{R} \\backslash \\mathcal{S}} \\left[ \\lambda sim_1(\\mathcal{Q}, D_i) - (1 - \\lambda) \\max_{D_j \\in \\mathcal{S}} \\left[ sim_2(D_i, D_j) \\right] \\right]\\right\\rbrace,$$\nwhere, $\\lambda$ is a hyperparameter that allows us to trade-off between relevance and diversity. Simply put, MMR selects a new document such that it has high relevance to the query (as measured using $sim_1$) and also diverges from the already selected documents in $\\mathcal{S}$ (as measured using $sim_2$). In our experiments, we set $k=100$, $\\lambda=0.7$, and $sim_1$ and $sim_2$ are cosine similarities between a $4$-gram and the image and another already selected $4$-gram respectively as measured using the pooled representations from CLIP.\nFigure 2 below shows a schematic of this approach.\nFigure 2. Finding meaningful $n$-grams from the curated dataset. This involves doing a nearest neighbor search using CLIP and reranking the nearest neighbors to ensure sufficient diversity among the top few items in the ranklist.\nGenerating prompts Having obtained a ranklist of text prompts such that the top few prompts are relevant and sufficiently diverse, we can compose the top-$m$ of these reranked top-$k$ items by simply concatenating them. More sophistacted approaches for combining these which retain grammatic correctness and better capture context of the remaining prompt are left as future work.\nIn our experiments, we found that $m$ can be used to weight different images in the prompt relative to each other. Figure 3 below demonstrates the effect of varying $m$ across images in the same prompt. Finally, $m = 4$ was found to be an average value and assigning weights to images relative to this value would change their relative importance for generation.\nFor example, the replacement prompt for the image in Figure 2 would be (with $m = 4$): \"tropical beach covered in water unsplash 4k photograph pastel palette matte painting pink at sunset\".\nResults We can perform the above procedure for each image in the prompt and convert our multimodal prompt to a text-only prompt. This translated text can then be used to prompt the Stable Diffusion model as originally intended for their txt2img variant.\nFigure 3 below shows some qualitative results from this method. Stable Diffusion is used to generate these images using the PLMS sampler over 200 steps.\nFigure 3. Qualitative results from our method. The prompts are shown on the left and 6 sampled generations are shown on the right. For the bottom 3 illustrations, we see the effect of varying $m$: weighting different images in the prompt reflects the corresponding details in the generation while staying true to the desired weighting.\nFuture Work Evaluation of the above work can be along the following fronts of (1) latency overhead when compared with the text-only version of the model and textual inversion for the same prompt, and (2) measuring the effectiveness of proposed retrieval algorithm; for example, given a text-image paired dataset, we can measure precision-recall scores for being able to retrieve $n$-grams belonging to the original pair.\nWhile this method is simple and effective, there are many potential directions for enabling multimodal prompting more deeply by changing how we condition the diffusion model:\nConditioning with aligned representations: The above method required to perform retrieval to handle the gap between the text representations and images representations from the prompts. If we train the image generation model to generate images by conditioning using language-image aligned representations, the prompt could be made to vary flexibly between images and text.\nTraining with multimodal prompts: Multimodal prompts can also be handled using a multimodal conditioning model with separate branches for different modalties. By including positional information alongwith the specific text/image inputs, we can allow the model to identify the sequential information and relationships across modalities. Note that this requires careful curation of a datset with paired (multimodal prompt, target image) pairs.\nReferences Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G. \u0026 Sutskever, I. (2021). Learning transferable visual models from natural language supervision. arXiv. https://arxiv.org/abs/2103.00020. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C. \u0026 Chen, M. (2022). Hierarchical text-conditional image generation with CLIP latents. arXiv. https://arxiv.org/abs/2204.06125. Rombach, R., Blattmann, A., Lorenz, D., Esser, P. \u0026 Ommer, B. (2021). High-resolution image synthesis with latent diffusion models. arXiv. https://arxiv.org/abs/2112.10752. Gafni, O., Polyak, A., Ashual, O., Sheynin, S., Parikh, D. \u0026 Taigman, Y. (2022). Make-a-scene: Scene-based text-to-image generation with human priors. arXiv. https://arxiv.org/abs/2203.13131. Carbonell, J. \u0026 Goldstein, J. (1998). The use of MMR, diversity-based reranking for reordering documents and producing summaries. Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval. Yu, J., Xu, Y., Koh, J., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J. \u0026 Wu, Y. (2022). Scaling autoregressive models for content-rich text-to-image generation. arXiv. https://arxiv.org/abs/2206.10789. Pressman, J., Crowson, K. \u0026 Contributors, S. (2022). Simulacra aesthetic captions (Version 1.0). Stability AI. https://github.com/JD-P/simulacra-aesthetic-captions. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A., Chechik, G. \u0026 Cohen-Or, D. (2022). An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv. https://arxiv.org/abs/2208.01618. ",
  "wordCount" : "2206",
  "inLanguage": "en",
  "datePublished": "2022-10-01T20:56:16-04:00",
  "dateModified": "2022-10-01T20:56:16-04:00",
  "author":{
    "@type": "Person",
    "name": "Nihal Jain"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://nihaljn.github.io/blog/multimodal-prompting/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Nihal Jain",
    "logo": {
      "@type": "ImageObject",
      "url": "https://nihaljn.github.io/columbia-favicon.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://nihaljn.github.io/" accesskey="h" title="Nihal Jain (Alt + H)">Nihal Jain</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://nihaljn.github.io/blog/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://nihaljn.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://nihaljn.github.io/publications/" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
            <li>
                <a href="https://nihaljn.github.io/shared/Nihal_Jain_CV_02-25.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Multimodal prompting for image generation
    </h1>
    <div class="post-description">
      Enabling the use of multiple modalities while prompting Stable Diffusion.
    </div>
    
    <div class="post-tags-meta">
      <a href="https://nihaljn.github.io/tags/multimodal-ml/" class="post-tag">multimodal-ml</a>
      <a href="https://nihaljn.github.io/tags/image-generation/" class="post-tag">image-generation</a>
      <a href="https://nihaljn.github.io/tags/diffusion-models/" class="post-tag">diffusion-models</a>
    </div>
    <div class="post-meta"><span title='2022-10-01 20:56:16 -0400 EDT'>October 1, 2022</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;Nihal Jain

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#method" aria-label="Method">Method</a><ul>
                        
                <li>
                    <a href="#data-curating-an-index-of-text-prompts" aria-label="Data: Curating an index of text prompts">Data: Curating an index of text prompts</a></li>
                <li>
                    <a href="#mapping-images-to-text" aria-label="Mapping images to text">Mapping images to text</a></li>
                <li>
                    <a href="#generating-prompts" aria-label="Generating prompts">Generating prompts</a></li></ul>
                </li>
                <li>
                    <a href="#results" aria-label="Results">Results</a></li>
                <li>
                    <a href="#future-work" aria-label="Future Work">Future Work</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><em>Note: Supporting code and data are available <a href="https://github.com/nihaljn/multimodal-prompting">here</a>.</em></p>
<figure class="align-center ">
    <img loading="lazy" src="images/einstein.png#center"
         alt="Figure 1. An illustration of Multimodal Prompting: an arbitrary composition of images and text can be used as a prompt to the Stable Diffusion model using the method presented in this article."/> <figcaption>
            <p>Figure 1. An illustration of Multimodal Prompting: an arbitrary composition of images and text can be used as a prompt to the Stable Diffusion model using the method presented in this article.</p>
        </figcaption>
</figure>

<p>In the past few months, we have experienced groundbreaking progress on the front of image generation models. While some of these models have been available behind black-box user interfaces (or only for fixed prompts) 




<span class="hugo-cite-intext" itemprop="citation">(<span class="hugo-cite-group">
        <a href="#parti"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Jiahui"><span itemprop="familyName">Yu</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Yuanzhong"><span itemprop="familyName">Xu</span></span>
                et al.,&#32;<span itemprop="datePublished">2022</span></a><span class="hugo-cite-citation"> 
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yu</span>,&#32;
        <meta itemprop="givenName" content="Jiahui" />
        J.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Xu</span>,&#32;
        <meta itemprop="givenName" content="Yuanzhong" />
        Y.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Koh</span>,&#32;
        <meta itemprop="givenName" content="Jing Yu" />
        J.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Luong</span>,&#32;
        <meta itemprop="givenName" content="Thang" />
        T.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Baid</span>,&#32;
        <meta itemprop="givenName" content="Gunjan" />
        G.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wang</span>,&#32;
        <meta itemprop="givenName" content="Zirui" />
        Z.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Vasudevan</span>,&#32;
        <meta itemprop="givenName" content="Vijay" />
        V.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ku</span>,&#32;
        <meta itemprop="givenName" content="Alexander" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yang</span>,&#32;
        <meta itemprop="givenName" content="Yinfei" />
        Y.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ayan</span>,&#32;
        <meta itemprop="givenName" content="Burcu Karagol" />
        B.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hutchinson</span>,&#32;
        <meta itemprop="givenName" content="Ben" />
        B.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Han</span>,&#32;
        <meta itemprop="givenName" content="Wei" />
        W.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Parekh</span>,&#32;
        <meta itemprop="givenName" content="Zarana" />
        Z.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Li</span>,&#32;
        <meta itemprop="givenName" content="Xin" />
        X.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhang</span>,&#32;
        <meta itemprop="givenName" content="Han" />
        H.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Baldridge</span>,&#32;
        <meta itemprop="givenName" content="Jason" />
        J.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wu</span>,&#32;
        <meta itemprop="givenName" content="Yonghui" />
        Y.</span>
    &#32;
      (<span itemprop="datePublished">2022</span>).
    &#32;<span itemprop="name">
      <i>Scaling autoregressive models for content-rich text-to-image generation</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">arXiv</span></span>.
    &#32;
    
    <a href="https://arxiv.org/abs/2206.10789"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://arxiv.org/abs/2206.10789</a>.</span>
  </span>
      </span>;&#32;<span class="hugo-cite-group">
        <a href="#dalle"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Aditya"><span itemprop="familyName">Ramesh</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Prafulla"><span itemprop="familyName">Dhariwal</span></span>
                et al.,&#32;<span itemprop="datePublished">2022</span></a><span class="hugo-cite-citation"> 
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ramesh</span>,&#32;
        <meta itemprop="givenName" content="Aditya" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Dhariwal</span>,&#32;
        <meta itemprop="givenName" content="Prafulla" />
        P.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Nichol</span>,&#32;
        <meta itemprop="givenName" content="Alex" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chu</span>,&#32;
        <meta itemprop="givenName" content="Casey" />
        C.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chen</span>,&#32;
        <meta itemprop="givenName" content="Mark" />
        M.</span>
    &#32;
      (<span itemprop="datePublished">2022</span>).
    &#32;<span itemprop="name">
      <i>Hierarchical text-conditional image generation with CLIP latents</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">arXiv</span></span>.
    &#32;
    
    <a href="https://arxiv.org/abs/2204.06125"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://arxiv.org/abs/2204.06125</a>.</span>
  </span>
      </span>;&#32;<span class="hugo-cite-group">
        <a href="#makeascene"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Oran"><span itemprop="familyName">Gafni</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Adam"><span itemprop="familyName">Polyak</span></span>
                et al.,&#32;<span itemprop="datePublished">2022</span></a><span class="hugo-cite-citation"> 
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gafni</span>,&#32;
        <meta itemprop="givenName" content="Oran" />
        O.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Polyak</span>,&#32;
        <meta itemprop="givenName" content="Adam" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ashual</span>,&#32;
        <meta itemprop="givenName" content="Oron" />
        O.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sheynin</span>,&#32;
        <meta itemprop="givenName" content="Shelly" />
        S.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Parikh</span>,&#32;
        <meta itemprop="givenName" content="Devi" />
        D.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Taigman</span>,&#32;
        <meta itemprop="givenName" content="Yaniv" />
        Y.</span>
    &#32;
      (<span itemprop="datePublished">2022</span>).
    &#32;<span itemprop="name">
      <i>Make-a-scene: Scene-based text-to-image generation with human priors</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">arXiv</span></span>.
    &#32;
    
    <a href="https://arxiv.org/abs/2203.13131"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://arxiv.org/abs/2203.13131</a>.</span>
  </span>
      </span>)</span>
, some of them 




<span class="hugo-cite-intext" itemprop="citation">(<span class="hugo-cite-group">
        <a href="#latentdiffusion"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Robin"><span itemprop="familyName">Rombach</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Andreas"><span itemprop="familyName">Blattmann</span></span>
                et al.,&#32;<span itemprop="datePublished">2021</span></a><span class="hugo-cite-citation"> 
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Rombach</span>,&#32;
        <meta itemprop="givenName" content="Robin" />
        R.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Blattmann</span>,&#32;
        <meta itemprop="givenName" content="Andreas" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Lorenz</span>,&#32;
        <meta itemprop="givenName" content="Dominik" />
        D.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Esser</span>,&#32;
        <meta itemprop="givenName" content="Patrick" />
        P.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ommer</span>,&#32;
        <meta itemprop="givenName" content="Björn" />
        B.</span>
    &#32;
      (<span itemprop="datePublished">2021</span>).
    &#32;<span itemprop="name">
      <i>High-resolution image synthesis with latent diffusion models</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">arXiv</span></span>.
    &#32;
    
    <a href="https://arxiv.org/abs/2112.10752"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://arxiv.org/abs/2112.10752</a>.</span>
  </span>
      </span>)</span>
 have been open-sourced with practically no restrictions! This has led to a tremendous surge in the use, analysis and extension of models like Stable Diffusion which is now being used for very creative applications such as infinite image outpainting, textual inversion, and so on. 




<span class="hugo-cite-intext" itemprop="citation">(<span class="hugo-cite-group">
        <a href="#textual-inversion"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Rinon"><span itemprop="familyName">Gal</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Yuval"><span itemprop="familyName">Alaluf</span></span>
                et al.,&#32;<span itemprop="datePublished">2022</span></a><span class="hugo-cite-citation"> 
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gal</span>,&#32;
        <meta itemprop="givenName" content="Rinon" />
        R.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Alaluf</span>,&#32;
        <meta itemprop="givenName" content="Yuval" />
        Y.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Atzmon</span>,&#32;
        <meta itemprop="givenName" content="Yuval" />
        Y.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Patashnik</span>,&#32;
        <meta itemprop="givenName" content="Or" />
        O.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bermano</span>,&#32;
        <meta itemprop="givenName" content="Amit H." />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chechik</span>,&#32;
        <meta itemprop="givenName" content="Gal" />
        G.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cohen-Or</span>,&#32;
        <meta itemprop="givenName" content="Daniel" />
        D.</span>
    &#32;
      (<span itemprop="datePublished">2022</span>).
    &#32;<span itemprop="name">
      <i>An image is worth one word: Personalizing text-to-image generation using textual inversion</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">arXiv</span></span>.
    &#32;
    
    <a href="https://arxiv.org/abs/2208.01618"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://arxiv.org/abs/2208.01618</a>.</span>
  </span>
      </span>)</span>
.</p>
<p>This article presents yet another extension to the Stable Diffusion model for image generation: a simple and fast strategy towards prompting Stable Diffusion with an arbitrary composition of image and text tokens. The use of multimodal prompts can be justified for the following reasons:</p>
<ul>
<li>Achieving high quality, desirable outputs from image generation models requires rigorous prompt tuning. While the community has reverse-engineered several interesting tricks for working with image generation models, this leads to a bad user-experience.</li>
<li>In zero-shot settings, when either the user or the model comes across a concept that is not within their vocabulary or too cumbersome to describe, the model may be better suited with processing the prompt if it can be &ldquo;shown&rdquo; the new concept and instructed to manipulate the new concept in textual format.</li>
</ul>
<p>Figure 1 shows an example of our method in action and more examples are shown below. In the following sections, we will look closer into how this can be achieved using a simple technique.</p>
<h2 id="method">Method<a hidden class="anchor" aria-hidden="true" href="#method">#</a></h2>
<p>Enabling multimodal prompting requires the underlying model to have a joint understanding of the different modalities. The straightforward solution for this would be to train or finetune Stable Diffusion so that its conditioning model can input multiple modalities, so that its outputs can be continued to use as before. Another method for this could be to use textual inversion 




<span class="hugo-cite-intext" itemprop="citation">(<span class="hugo-cite-group">
        <a href="#textual-inversion"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Rinon"><span itemprop="familyName">Gal</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Yuval"><span itemprop="familyName">Alaluf</span></span>
                et al.,&#32;<span itemprop="datePublished">2022</span></a><span class="hugo-cite-citation"> 
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gal</span>,&#32;
        <meta itemprop="givenName" content="Rinon" />
        R.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Alaluf</span>,&#32;
        <meta itemprop="givenName" content="Yuval" />
        Y.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Atzmon</span>,&#32;
        <meta itemprop="givenName" content="Yuval" />
        Y.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Patashnik</span>,&#32;
        <meta itemprop="givenName" content="Or" />
        O.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bermano</span>,&#32;
        <meta itemprop="givenName" content="Amit H." />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chechik</span>,&#32;
        <meta itemprop="givenName" content="Gal" />
        G.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cohen-Or</span>,&#32;
        <meta itemprop="givenName" content="Daniel" />
        D.</span>
    &#32;
      (<span itemprop="datePublished">2022</span>).
    &#32;<span itemprop="name">
      <i>An image is worth one word: Personalizing text-to-image generation using textual inversion</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">arXiv</span></span>.
    &#32;
    
    <a href="https://arxiv.org/abs/2208.01618"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://arxiv.org/abs/2208.01618</a>.</span>
  </span>
      </span>)</span>
: we can finetune the model to represent each of the non-text concepts in the prompt as textual tokens, and use the newly learned tokens to replace the images in the prompts. However, in this article, we will look at a simpler strategy that requires no training or finetuning in any way, with negligible additional overhead in comparison to the original Stable Diffusion.</p>
<p>The high level overview of our method is as follows:</p>
<ul>
<li>
<p>Maintain a rich corpus of prompts (or sub-prompts) that can textually explain various aspects of the images in any prompt in detail;</p>
</li>
<li>
<p>For each image in the prompt, find the nearest neighboring text prompts in the above dataset by projecting the image and the above curated text prompts into the pooler representation of a text-image alignment model, such as CLIP 




<span class="hugo-cite-intext" itemprop="citation">(<span class="hugo-cite-group">
        <a href="#clip"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Alec"><span itemprop="familyName">Radford</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Jong Wook"><span itemprop="familyName">Kim</span></span>
                et al.,&#32;<span itemprop="datePublished">2021</span></a><span class="hugo-cite-citation"> 
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Radford</span>,&#32;
        <meta itemprop="givenName" content="Alec" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kim</span>,&#32;
        <meta itemprop="givenName" content="Jong Wook" />
        J.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hallacy</span>,&#32;
        <meta itemprop="givenName" content="Chris" />
        C.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ramesh</span>,&#32;
        <meta itemprop="givenName" content="Aditya" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Goh</span>,&#32;
        <meta itemprop="givenName" content="Gabriel" />
        G.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Agarwal</span>,&#32;
        <meta itemprop="givenName" content="Sandhini" />
        S.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sastry</span>,&#32;
        <meta itemprop="givenName" content="Girish" />
        G.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Askell</span>,&#32;
        <meta itemprop="givenName" content="Amanda" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Mishkin</span>,&#32;
        <meta itemprop="givenName" content="Pamela" />
        P.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Clark</span>,&#32;
        <meta itemprop="givenName" content="Jack" />
        J.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Krueger</span>,&#32;
        <meta itemprop="givenName" content="Gretchen" />
        G.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sutskever</span>,&#32;
        <meta itemprop="givenName" content="Ilya" />
        I.</span>
    &#32;
      (<span itemprop="datePublished">2021</span>).
    &#32;<span itemprop="name">
      <i>Learning transferable visual models from natural language supervision</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">arXiv</span></span>.
    &#32;
    
    <a href="https://arxiv.org/abs/2103.00020"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://arxiv.org/abs/2103.00020</a>.</span>
  </span>
      </span>)</span>
;</p>
</li>
<li>
<p>A meaningful prompt can be obtained by concatenating a diverse set of nearest neighbors obtained: this is achieved by using a popular reranking algorithm, Maximum Marginal Relevance (MMR) 




<span class="hugo-cite-intext" itemprop="citation">(<span class="hugo-cite-group">
        <a href="#mmr"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Jaime"><span itemprop="familyName">Carbonell</span></span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Jade"><span itemprop="familyName">Goldstein</span></span>,&#32;<span itemprop="datePublished">1998</span></a><span class="hugo-cite-citation"> 
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Carbonell</span>,&#32;
        <meta itemprop="givenName" content="Jaime" />
        J.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Goldstein</span>,&#32;
        <meta itemprop="givenName" content="Jade" />
        J.</span>
    &#32;
      (<span itemprop="datePublished">1998</span>).
    &#32;<span itemprop="name">
      <i>The use of MMR, diversity-based reranking for reordering documents and producing summaries</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval</span></span>.
    </span>
  </span>
      </span>)</span>
 over the nearest neighbors found above;</p>
</li>
<li>
<p>Replace each image in the prompt with its &ldquo;translated&rdquo; text to obtain a text-only prompt which can be used to prompt Stable Diffusion without any finetuning or training.</p>
</li>
</ul>
<p>Let us look at each piece of this method in more detail.</p>
<h3 id="data-curating-an-index-of-text-prompts">Data: Curating an index of text prompts<a hidden class="anchor" aria-hidden="true" href="#data-curating-an-index-of-text-prompts">#</a></h3>
<p>Our method requires being able to retrieve text prompts that can adequately describe any image used in a prompt. Towards this, we use a processed version of Simulacra Aesthetic Captions 




<span class="hugo-cite-intext" itemprop="citation">(<span class="hugo-cite-group">
        <a href="#simulacra"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="John David"><span itemprop="familyName">Pressman</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Katherine"><span itemprop="familyName">Crowson</span></span>
                et al.,&#32;<span itemprop="datePublished">2022</span></a><span class="hugo-cite-citation"> 
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/Report"
        data-type="report"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Pressman</span>,&#32;
        <meta itemprop="givenName" content="John David" />
        J.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Crowson</span>,&#32;
        <meta itemprop="givenName" content="Katherine" />
        K.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Contributors</span>,&#32;
        <meta itemprop="givenName" content="Simulacra Captions" />
        S.</span>
    &#32;
      (<span itemprop="datePublished">2022</span>).
    &#32;<span itemprop="name">
      <i>Simulacra aesthetic captions</i>
    </span><span itemprop="reportNumber">&nbsp;(Version 1.0)</span>.&nbsp;<span itemprop="Organization"
          itemtype="http://schema.org/Organization"
          itemscope>
      <span itemprop="name">Stability AI.</span> 
    
    &#32;
    
    <a href="https://github.com/JD-P/simulacra-aesthetic-captions"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://github.com/JD-P/simulacra-aesthetic-captions</a>.</span>
  
  
  
  </span>
      </span>)</span>
 which is a curated dataset of over 40K prompts written by users of image-generation models.</p>
<p>Most prompts in this datset are long-winded and highly detailed such that they only describe their respective generated images. Thus, this set of prompts may not be able to describe a new image that has details different from any image that this dataset describes. So, we chunk each prompt in the dataset into smaller $n$-grams (in our experiments, we fix $n = 4$) such that each $n$-gram can be expected to describe only an atomic detail of some image. Thus, in expectation, combining different $n$-grams across the dataset should expand the scope of images describable by the dataset. While blind composition of random $n$-grams from the dataset may not lead to prompts that are intelligible or even grammarly correct, we can expect our retrieval strategy to obtain a diverse set of terms in the composition such that each provides meaningful signal to Stable Diffusion about what has to be generated.</p>
<p>For example, the prompt <code>'A crowded subway car, the fluorescent lights flickering and casting an eerie glow on the people inside'</code> will be mapped to a list of $4$-grams as: <code>['A crowded subway car', 'crowded subway car, the', 'subway car, the fluorescent', ..., 'on the people inside']</code>.</p>
<h3 id="mapping-images-to-text">Mapping images to text<a hidden class="anchor" aria-hidden="true" href="#mapping-images-to-text">#</a></h3>
<p>Given any image, we are interested in retrieving a set of $n$-grams such that composing them adequately describes the image. An intuitive way for doing this is to find the nearest neighbors texts to a query image in the representation space of a text-image alignment model. So, we utilize CLIP&rsquo;s pooled representation space to perform cross-modal nearest-neighbor search where the query is the image to be descibed and the index of candidates comprises all the $n$-grams curated above.</p>
<p>Simply picking the top-$k$ neighbors from a nearest-neighbor search described above might lead to uninteresting results. For example, if the top-$k$ text neighbors describe the same aspect of an image, the resulting combination of them will not sufficiently describe the query.</p>
<p>Towards mitigating this, we use a popular re-ranking algorithm, called Maximum Marginal Relevance (MMR) to pick a diverse set of $n$-grams from the top-$k$ neighbors, which when concatenated together, describe the image more meaningfully. MMR iteratively selects documents from the top-$k$ neighbors, such that each new selected document introduces new information while still staying relevant to the query. Specifically, given the collection of top-$k$ documents, $\mathcal{R}$, a set of already selected documents, $\mathcal{S}$, and a query $\mathcal{Q}$, MMR augments the set of selected documents as:</p>
<p>$$\mathcal{S} \leftarrow \mathcal{S}\ \  \cup\ \ \left\lbrace\argmax_{D_i \in \mathcal{R} \backslash \mathcal{S}} \left[ \lambda sim_1(\mathcal{Q}, D_i) - (1 - \lambda) \max_{D_j \in \mathcal{S}} \left[ sim_2(D_i, D_j) \right] \right]\right\rbrace,$$</p>
<p>where, $\lambda$ is a hyperparameter that allows us to trade-off between relevance and diversity. Simply put, MMR selects a new document such that it has high relevance to the query (as measured using $sim_1$) and also diverges from the already selected documents in $\mathcal{S}$ (as measured using $sim_2$). In our experiments, we set $k=100$, $\lambda=0.7$, and $sim_1$ and $sim_2$ are cosine similarities between a $4$-gram and the image and another already selected $4$-gram respectively as measured using the pooled representations from CLIP.</p>
<p>Figure 2 below shows a schematic of this approach.</p>
<figure class="align-center ">
    <img loading="lazy" src="images/search_schematic.png#center"
         alt="Figure 2. Finding meaningful $n$-grams from the curated dataset. This involves doing a nearest neighbor search using CLIP and reranking the nearest neighbors to ensure sufficient diversity among the top few items in the ranklist."/> <figcaption>
            <p>Figure 2. Finding meaningful $n$-grams from the curated dataset. This involves doing a nearest neighbor search using CLIP and reranking the nearest neighbors to ensure sufficient diversity among the top few items in the ranklist.</p>
        </figcaption>
</figure>

<h3 id="generating-prompts">Generating prompts<a hidden class="anchor" aria-hidden="true" href="#generating-prompts">#</a></h3>
<p>Having obtained a ranklist of text prompts such that the top few prompts are relevant and sufficiently diverse, we can compose the top-$m$ of these reranked top-$k$ items by simply concatenating them. More sophistacted approaches for combining these which retain grammatic correctness and better capture context of the remaining prompt are left as future work.</p>
<p>In our experiments, we found that $m$ can be used to weight different images in the prompt relative to each other. Figure 3 below demonstrates the effect of varying $m$ across images in the same prompt. Finally, $m = 4$ was found to be an average value and assigning weights to images relative to this value would change their relative importance for generation.</p>
<p>For example, the replacement prompt for the image in Figure 2 would be (with $m = 4$): <code>&quot;tropical beach covered in water unsplash 4k photograph pastel palette matte painting pink at sunset&quot;</code>.</p>
<h2 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h2>
<p>We can perform the above procedure for each image in the prompt and convert our multimodal prompt to a text-only prompt. This translated text can then be used to prompt the Stable Diffusion model as originally intended for their txt2img variant.</p>
<p>Figure 3 below shows some qualitative results from this method. Stable Diffusion is used to generate these images using the PLMS sampler over 200 steps.</p>
<figure>
    <img loading="lazy" src="images/tiger.png"/> 
</figure>

<figure>
    <img loading="lazy" src="images/cyber1.png"/> 
</figure>

<figure>
    <img loading="lazy" src="images/cyber2.png"/> 
</figure>

<figure class="align-center ">
    <img loading="lazy" src="images/cyber3.png#center"
         alt="Figure 3. Qualitative results from our method. The prompts are shown on the left and 6 sampled generations are shown on the right. For the bottom 3 illustrations, we see the effect of varying $m$: weighting different images in the prompt reflects the corresponding details in the generation while staying true to the desired weighting."/> <figcaption>
            <p>Figure 3. Qualitative results from our method. The prompts are shown on the left and 6 sampled generations are shown on the right. For the bottom 3 illustrations, we see the effect of varying $m$: weighting different images in the prompt reflects the corresponding details in the generation while staying true to the desired weighting.</p>
        </figcaption>
</figure>

<h2 id="future-work">Future Work<a hidden class="anchor" aria-hidden="true" href="#future-work">#</a></h2>
<p>Evaluation of the above work can be along the following fronts of (1) latency overhead when compared with the text-only version of the model and textual inversion for the same prompt, and (2) measuring the effectiveness of proposed retrieval algorithm; for example, given a text-image paired dataset, we can measure precision-recall scores for being able to retrieve $n$-grams belonging to the original pair.</p>
<p>While this method is simple and effective, there are many potential directions for enabling multimodal prompting more deeply by changing how we condition the diffusion model:</p>
<ul>
<li>
<p><strong>Conditioning with aligned representations</strong>: The above method required to perform retrieval to handle the gap between the text representations and images representations from the prompts. If we train the image generation model to generate images by conditioning using language-image aligned representations, the prompt could be made to vary flexibly between images and text.</p>
</li>
<li>
<p><strong>Training with multimodal prompts</strong>: Multimodal prompts can also be handled using a multimodal conditioning model with separate branches for different modalties. By including positional information alongwith the specific text/image inputs, we can allow the model to identify the sequential information and relationships across modalities. Note that this requires careful curation of a datset with paired (multimodal prompt, target image) pairs.</p>
</li>
</ul>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>

  
  
  
  
  
  
  
  
  
  
  
  
  <section class="hugo-cite-bibliography">
    <dl>
      
  
        <div id="clip">
  
          <dd>
            
            <p style="text-indent: -30px; margin-left: 30px">
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Radford</span>,&#32;
        <meta itemprop="givenName" content="Alec" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kim</span>,&#32;
        <meta itemprop="givenName" content="Jong Wook" />
        J.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hallacy</span>,&#32;
        <meta itemprop="givenName" content="Chris" />
        C.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ramesh</span>,&#32;
        <meta itemprop="givenName" content="Aditya" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Goh</span>,&#32;
        <meta itemprop="givenName" content="Gabriel" />
        G.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Agarwal</span>,&#32;
        <meta itemprop="givenName" content="Sandhini" />
        S.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sastry</span>,&#32;
        <meta itemprop="givenName" content="Girish" />
        G.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Askell</span>,&#32;
        <meta itemprop="givenName" content="Amanda" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Mishkin</span>,&#32;
        <meta itemprop="givenName" content="Pamela" />
        P.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Clark</span>,&#32;
        <meta itemprop="givenName" content="Jack" />
        J.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Krueger</span>,&#32;
        <meta itemprop="givenName" content="Gretchen" />
        G.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sutskever</span>,&#32;
        <meta itemprop="givenName" content="Ilya" />
        I.</span>
    &#32;
      (<span itemprop="datePublished">2021</span>).
    &#32;<span itemprop="name">
      <i>Learning transferable visual models from natural language supervision</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">arXiv</span></span>.
    &#32;
    
    <a href="https://arxiv.org/abs/2103.00020"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://arxiv.org/abs/2103.00020</a>.</span>
  </dd>
            </p>
  
        </div>
  
        <div id="dalle">
  
          <dd>
            
            <p style="text-indent: -30px; margin-left: 30px">
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ramesh</span>,&#32;
        <meta itemprop="givenName" content="Aditya" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Dhariwal</span>,&#32;
        <meta itemprop="givenName" content="Prafulla" />
        P.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Nichol</span>,&#32;
        <meta itemprop="givenName" content="Alex" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chu</span>,&#32;
        <meta itemprop="givenName" content="Casey" />
        C.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chen</span>,&#32;
        <meta itemprop="givenName" content="Mark" />
        M.</span>
    &#32;
      (<span itemprop="datePublished">2022</span>).
    &#32;<span itemprop="name">
      <i>Hierarchical text-conditional image generation with CLIP latents</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">arXiv</span></span>.
    &#32;
    
    <a href="https://arxiv.org/abs/2204.06125"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://arxiv.org/abs/2204.06125</a>.</span>
  </dd>
            </p>
  
        </div>
  
        <div id="latentdiffusion">
  
          <dd>
            
            <p style="text-indent: -30px; margin-left: 30px">
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Rombach</span>,&#32;
        <meta itemprop="givenName" content="Robin" />
        R.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Blattmann</span>,&#32;
        <meta itemprop="givenName" content="Andreas" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Lorenz</span>,&#32;
        <meta itemprop="givenName" content="Dominik" />
        D.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Esser</span>,&#32;
        <meta itemprop="givenName" content="Patrick" />
        P.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ommer</span>,&#32;
        <meta itemprop="givenName" content="Björn" />
        B.</span>
    &#32;
      (<span itemprop="datePublished">2021</span>).
    &#32;<span itemprop="name">
      <i>High-resolution image synthesis with latent diffusion models</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">arXiv</span></span>.
    &#32;
    
    <a href="https://arxiv.org/abs/2112.10752"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://arxiv.org/abs/2112.10752</a>.</span>
  </dd>
            </p>
  
        </div>
  
        <div id="makeascene">
  
          <dd>
            
            <p style="text-indent: -30px; margin-left: 30px">
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gafni</span>,&#32;
        <meta itemprop="givenName" content="Oran" />
        O.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Polyak</span>,&#32;
        <meta itemprop="givenName" content="Adam" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ashual</span>,&#32;
        <meta itemprop="givenName" content="Oron" />
        O.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sheynin</span>,&#32;
        <meta itemprop="givenName" content="Shelly" />
        S.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Parikh</span>,&#32;
        <meta itemprop="givenName" content="Devi" />
        D.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Taigman</span>,&#32;
        <meta itemprop="givenName" content="Yaniv" />
        Y.</span>
    &#32;
      (<span itemprop="datePublished">2022</span>).
    &#32;<span itemprop="name">
      <i>Make-a-scene: Scene-based text-to-image generation with human priors</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">arXiv</span></span>.
    &#32;
    
    <a href="https://arxiv.org/abs/2203.13131"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://arxiv.org/abs/2203.13131</a>.</span>
  </dd>
            </p>
  
        </div>
  
        <div id="mmr">
  
          <dd>
            
            <p style="text-indent: -30px; margin-left: 30px">
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Carbonell</span>,&#32;
        <meta itemprop="givenName" content="Jaime" />
        J.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Goldstein</span>,&#32;
        <meta itemprop="givenName" content="Jade" />
        J.</span>
    &#32;
      (<span itemprop="datePublished">1998</span>).
    &#32;<span itemprop="name">
      <i>The use of MMR, diversity-based reranking for reordering documents and producing summaries</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval</span></span>.
    </span>
  </dd>
            </p>
  
        </div>
  
        <div id="parti">
  
          <dd>
            
            <p style="text-indent: -30px; margin-left: 30px">
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yu</span>,&#32;
        <meta itemprop="givenName" content="Jiahui" />
        J.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Xu</span>,&#32;
        <meta itemprop="givenName" content="Yuanzhong" />
        Y.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Koh</span>,&#32;
        <meta itemprop="givenName" content="Jing Yu" />
        J.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Luong</span>,&#32;
        <meta itemprop="givenName" content="Thang" />
        T.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Baid</span>,&#32;
        <meta itemprop="givenName" content="Gunjan" />
        G.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wang</span>,&#32;
        <meta itemprop="givenName" content="Zirui" />
        Z.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Vasudevan</span>,&#32;
        <meta itemprop="givenName" content="Vijay" />
        V.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ku</span>,&#32;
        <meta itemprop="givenName" content="Alexander" />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yang</span>,&#32;
        <meta itemprop="givenName" content="Yinfei" />
        Y.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ayan</span>,&#32;
        <meta itemprop="givenName" content="Burcu Karagol" />
        B.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hutchinson</span>,&#32;
        <meta itemprop="givenName" content="Ben" />
        B.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Han</span>,&#32;
        <meta itemprop="givenName" content="Wei" />
        W.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Parekh</span>,&#32;
        <meta itemprop="givenName" content="Zarana" />
        Z.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Li</span>,&#32;
        <meta itemprop="givenName" content="Xin" />
        X.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhang</span>,&#32;
        <meta itemprop="givenName" content="Han" />
        H.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Baldridge</span>,&#32;
        <meta itemprop="givenName" content="Jason" />
        J.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wu</span>,&#32;
        <meta itemprop="givenName" content="Yonghui" />
        Y.</span>
    &#32;
      (<span itemprop="datePublished">2022</span>).
    &#32;<span itemprop="name">
      <i>Scaling autoregressive models for content-rich text-to-image generation</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">arXiv</span></span>.
    &#32;
    
    <a href="https://arxiv.org/abs/2206.10789"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://arxiv.org/abs/2206.10789</a>.</span>
  </dd>
            </p>
  
        </div>
  
        <div id="simulacra">
  
          <dd>
            
            <p style="text-indent: -30px; margin-left: 30px">
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/Report"
        data-type="report"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Pressman</span>,&#32;
        <meta itemprop="givenName" content="John David" />
        J.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Crowson</span>,&#32;
        <meta itemprop="givenName" content="Katherine" />
        K.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Contributors</span>,&#32;
        <meta itemprop="givenName" content="Simulacra Captions" />
        S.</span>
    &#32;
      (<span itemprop="datePublished">2022</span>).
    &#32;<span itemprop="name">
      <i>Simulacra aesthetic captions</i>
    </span><span itemprop="reportNumber">&nbsp;(Version 1.0)</span>.&nbsp;<span itemprop="Organization"
          itemtype="http://schema.org/Organization"
          itemscope>
      <span itemprop="name">Stability AI.</span> 
    
    &#32;
    
    <a href="https://github.com/JD-P/simulacra-aesthetic-captions"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://github.com/JD-P/simulacra-aesthetic-captions</a>.</span>
  
  
  
  </dd>
            </p>
  
        </div>
  
        <div id="textual-inversion">
  
          <dd>
            
            <p style="text-indent: -30px; margin-left: 30px">
  
  
  
  
  
  
  
  
  
  
  <span itemscope
        itemtype="https://schema.org/CreativeWork"
        data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gal</span>,&#32;
        <meta itemprop="givenName" content="Rinon" />
        R.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Alaluf</span>,&#32;
        <meta itemprop="givenName" content="Yuval" />
        Y.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Atzmon</span>,&#32;
        <meta itemprop="givenName" content="Yuval" />
        Y.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Patashnik</span>,&#32;
        <meta itemprop="givenName" content="Or" />
        O.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bermano</span>,&#32;
        <meta itemprop="givenName" content="Amit H." />
        A.</span>,&#32;
        <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chechik</span>,&#32;
        <meta itemprop="givenName" content="Gal" />
        G.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cohen-Or</span>,&#32;
        <meta itemprop="givenName" content="Daniel" />
        D.</span>
    &#32;
      (<span itemprop="datePublished">2022</span>).
    &#32;<span itemprop="name">
      <i>An image is worth one word: Personalizing text-to-image generation using textual inversion</i></span>.
    &#32;
    <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
      <span itemprop="name">arXiv</span></span>.
    &#32;
    
    <a href="https://arxiv.org/abs/2208.01618"
       itemprop="identifier"
       itemtype="https://schema.org/URL">https://arxiv.org/abs/2208.01618</a>.</span>
  </dd>
            </p>
  
        </div>
    </dl>
  </section>
  

<!-- ## References

[1] Jaime Carbonell and Jade Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335–336, 1998.

[2] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors, 2022.

[3] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022.

[4] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022.

[5] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021.

[6] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation, 2022.

[7] John David Pressman, Katherine Crowson, and Simulacra Captions Contributors. Simulacra aesthetic captions. Technical Report Version 1.0, Stability AI, 2022. url https://github.com/JD-P/simulacra-aesthetic-captions . -->

  </div>

  <footer class="post-footer">
    
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://nihaljn.github.io/">Nihal Jain</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><script>
document.addEventListener("DOMContentLoaded", function() {
    
    const footerSpans = document.querySelectorAll("footer.footer span");
    footerSpans.forEach(span => {
    if (span.textContent.includes("Powered by")) {
        
        span.innerHTML = span.innerHTML.replace("Powered by", "Theme based on");
    }
    });
});
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
