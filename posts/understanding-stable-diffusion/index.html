<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Understanding Stable Diffusion | Nihal Jain</title>
<meta name="keywords" content="">
<meta name="description" content="A deep dive into the method and code of Stable Diffusion.">
<meta name="author" content="Nihal Jain">
<link rel="canonical" href="https://nihaljn.github.io/posts/understanding-stable-diffusion/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e7d32d5d4e3e0d44282961dc2052290d7f8160c06aca3ec5f6d04c4714b48c47.css" integrity="sha256-59MtXU4&#43;DUQoKWHcIFIpDX&#43;BYMBqyj7F9tBMRxS0jEc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://nihaljn.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://nihaljn.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://nihaljn.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://nihaljn.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://nihaljn.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>




<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script><meta property="og:title" content="Understanding Stable Diffusion" />
<meta property="og:description" content="A deep dive into the method and code of Stable Diffusion." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://nihaljn.github.io/posts/understanding-stable-diffusion/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-01T20:53:39-04:00" />
<meta property="article:modified_time" content="2022-10-01T20:53:39-04:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Understanding Stable Diffusion"/>
<meta name="twitter:description" content="A deep dive into the method and code of Stable Diffusion."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://nihaljn.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Understanding Stable Diffusion",
      "item": "https://nihaljn.github.io/posts/understanding-stable-diffusion/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Understanding Stable Diffusion",
  "name": "Understanding Stable Diffusion",
  "description": "A deep dive into the method and code of Stable Diffusion.",
  "keywords": [
    
  ],
  "articleBody": "This article serves to explain the Stable Diffusion [7] model and some of its implementation details. Note: The Stable Diffusion model consists of several blocks carefully engineered together in a large code-base. Towards maintaining ease of simplicity, this article skips over unnecessary lines of code or modifies the code in some way without altering its logic.\nArchitecture Details Figure 1. The Stable Diffusion model consists of multiple stages across several blocks as shown here. The figure shows one possible data flow path labelled in roman numerals $ I - IV $ in that order; this sequence also resembles the training of the model. The latent representation of an encoded image is gradually randomized by injecting noise in a Markov chain. This is followed by predicting the noise (and the denoised latent representation) conditioned on some information about what has to be recovered – in this case, a text description of the original image. Finally, the predicted denoised latent representation can be decoded to reconstruct the original image.\nAs shown in Figure 1, there are several pieces to the Stable Diffusion model. In the following sections, we will look into each block in more detail alongside the code snippets that make these blocks work.\nAutoencoder The autoencoder used in Stable Diffusion is similar to that in the VQGAN paper [1]. It serves the purpose of perceptual compression, and is trained using a perceptual loss and a patch-based adversarial objective. The authors argue in the paper that together, these objectives are responsible for learning semantic variation and instilling realism in the generations. Additionally, there exist two options for a regularization objective to ensure that the learned latent space distribution is zero-centered and has low variance. All objectives used for training the autoencoder are explained in more detail in section: [training details \u003e autoencoder].\nIf the regularization objective used is KL divergence between the predicted distribution and $\\mathcal{N}(0, 1)$, we’d be training the AutoencoderKL which we will understand in detail. AutoencoderKL is defined as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 '''Source: ldm/models/autoencoder.py''' class AutoencoderKL(pl.LightningModule): def __init__(self, modelconfig, lossconfig, embed_dim): super().__init__() self.embed_dim = embed_dim self.encoder = Encoder(**modelconfig) self.decoder = Decoder(**modelconfig) self.loss = LPIPSWithDiscriminator(lossconfig) self.quant_conv = torch.nn.Conv2d(2*modelconfig[\"z_channels\"], 2*embed_dim, 1) self.post_quant_conv = torch.nn.Conv2d(embed_dim, modelconfig[\"z_channels\"], 1) # skipped irrelevant lines def encode(self, x): h = self.encoder(x) moments = self.quant_conv(h) posterior = DiagonalGaussianDistribution(moments) return posterior def decode(self, z): z = self.post_quant_conv(z) dec = self.decoder(z) return dec def forward(self, input, sample_posterior=True): posterior = self.encode(input) z = posterior.sample() dec = self.decode(z) return dec, posterior The encoding phase of the autoencoder outputs a posterior over the latent space given an input image x. The posterior is assumed to have a diagonal covarinace matrix, so the posterior can be identified using 2*embed_dim parameters (the means and variances of all dimensions). Then, a sample z from the posterior is decoded to obtain a prediction of the original image in the same space as x. This model is optimized using LPIPSWithDiscriminator which includes a perceptual loss, an adversarial loss and a regularizing KL divergence loss; we shall return to this later.\nFurther, we note here that the same z undergoes the diffusion process to obtain a noisy z so that we can train a denoising diffusion probabilistic model to estimate the denoised version given some conditioning.\nThe Encoder and Decoder defined above comprise well-known modules in deep learning such as residual blocks [2] and self-attention [10] in an intuitive architecture. So, we skip a detailed discussion of their implementations here which can be found at ldm/modules/diffusionmodules/model.py.\nDiffusion Model The diffusion model in Stable Diffusion has a U-Net architecture [8] with support for processing timestep embeddings, and cross-attention between the context embedding and the latent representation to be denoised.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 '''Source: ldm/modules/diffusionmodules/openaimodel.py''' class UNetModel(torch.nn.Module): def __init__(self): super().__init__() time_embed_dim = model_channels * 4 self.time_embed = nn.Sequential( linear(model_channels, time_embed_dim), nn.SiLU(), linear(time_embed_dim, time_embed_dim), ) self.input_blocks = [torch.nn.Conv2d(in_channels, model_channels, 3)] for level, mult in enumerate(channel_mult): for _ in range(num_res_blocks): self.input_blocks += [ ResBlock(), # processes z and timestep embed AttentionBlock(), # cross-attn between z and context embed ] # skipped arguments to modules self.input_blocks += [ResBlock()] # processes z and timestep embed self.input_blocks = torch.nn.Sequential(*self.input_blocks) self.middle_block = torch.nn.Sequential(*[ ResBlock(), # processes z and timestep embed AttentionBlock(), # cross-attn between z and context embed ResBlock() # processes z and timestep embed ]) # skipped arguments to modules self.output_blocks = [] for level, mult in enumerate(channel_mult): for _ in range(num_res_blocks): self.output_blocks += [ ResBlock(), # processes z, skip conn from inp blocks, timestep embed AttentionBlock() # cross-attn between z and context embed ] # skipped arguments to modules self.input_blocks += [ResBlock()] # processes z and timestep embed self.output_blocks = torch.nn.Sequential(*self.output_blocks) self.out = nn.Sequential( normalization(ch), nn.SiLU(), torch.nn.Conv2d(model_channels, out_channels, 3), ) Similar to the autoencoder, the U-Net model comprises some well-known techniques. First, we see an MLP head to project timestep information to a high-dimensional embedding space. The input, middle and output blocks are composed of residual and cross-attention blocks which enables the generative model to lern a conditional distribution for the reverse diffusion process. Finally, the output blocks also involve skip connections from the input blocks, a feature typical to U-Net style architectures.\nConditioning Model The Stable Diffusion models are paramaterized to be able to perform both conditional and non-conditional image generation. Towards conditional image generation, we would require a mechanism to encode the human-parsable conditioning such as text or image to be fed into the cross-attention layers of the U-Net above. So, the authors have designed domain-specific conditioning models for the various data domains that they train their models on:\nText-to-image generation: At the time of writing of this article, Stable Diffusion’s text-to-image generation uses text embeddings from a frozen CLIP [6] model to obtain conditioning embeddings.\nClass-conditional generation: In order to condition on class-information, for example, classes of ImageNet, they use a single embedding layer to transform a class index to its corresponding representation.\nTraining Details The Stable Diffusion model is trained in two stages: (1) training the autoencoder alone, i.e., $I, IV$ only in figure 1, and (2) training the diffusion model alone after fixing the autoencoder, i.e., $I - IV$ in figure 1 but keeping $I, IV$ frozen. Let’s look at each phase in more detail.\nAutoencoder The autoencoder is trained using the LPIPSWithDiscriminator. This loss has the following components:\nReconstruction Loss ($\\mathcal{L}_{r}$): The mean-squared-error between the reconstructions and the predicted images in the pixel space\nPerceptual Loss ($\\mathcal{L}_{p}$): The Learned Perceptual Image Patch Similarity (LPIPS) metric loss. In a nutshell, this objective tries to minimize the perceptual differences between the reconstructed and original images by making sure that the VGG features of both the images are close to each other. This is based on the empirical finding that differences in the representation spaces of computer vision models such as VGG capture perceptual distance very well compared to differences in image space or using metrics like PSNR [12].\nKL divergence Loss ($\\mathcal{L}_{k}$): This is a regularization term added to the objective of the autoencoder to ensure that the latent dimension of the autoencoder is zero-centered and has low variance. Towards this the KL divergence is measure with respect to $\\mathcal{N}(0, 1)$.\nAdversarial Loss ($\\mathcal{L}_{a}$): The autoencoder is also trained against a Patch-GAN [patchgan] discriminator, i.e., the autoencoder is trained to maximize the scores of a discriminator which looks at various patches of any image to make a collective decision about whether the image is real or fake.\nThe adversarial loss is scaled with an adaptive weight such that there is a balance between the adversarial and reconstruction objectives [1]:\n$$ \\lambda_{adv} = \\frac{\\nabla \\mathcal{L}_r + \\mathcal{L}_p}{\\nabla \\mathcal{L}_a + \\delta} $$\nIntuitively, if the gradients of $\\mathcal{L}_a$ are small, then we should scale up the loss appropriately to allow faster training.\nDiffusion Model Having trained the autoencoder, we can train the diffusion model to denoise noisy latent representations from the autoencoder. This involves gradually injecting noise into the outputs of the encoder, and training a diffusion model to learn the reverse process of this noise injection.\nThe noise is injected into the latent representation in a Markov chain of $T$ time steps. Given a representation at some time step $t$, we can obtain the next time step noisy representation as:\n$$ q(x_{t + 1} | x_t) = \\mathcal{N}(\\sqrt{1 - \\beta_t}x_{t - 1}, \\beta_t \\mathbf{I}) $$\nIt can be shown that we can skip over time steps instead of iterating over each time step as:\n$$ q(x_{t} | x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha_t}}x_0, (1 - \\bar{\\alpha_t})\\mathbf{I}) $$\nThis is performed in code as:\n1 2 3 4 5 6 7 '''Source: ldm/models/diffusion/ddpm.py''' def q_sample(self, x_start, t): noise = torch.randn_like(x_start) return ( self.sqrt_alphas_cumprod[t] * x_start + self.sqrt_one_minus_alphas_cumprod[t] * noise ) The reverse diffusion process can be shown to be:\n$$ q(x_{t - 1} | x_t, x_0) = \\mathcal{N}(\\tilde{\\mu}(x_t, x_0), \\tilde{\\beta}_t) $$\nThis is performed in code as:\n1 2 3 4 5 6 7 '''Source: ldm/models/diffusion/ddpm.py''' def q_posterior(self, x_start, x_t, t): posterior_mean = ( self.posterior_mean_coef1[t] * x_start + self.posterior_mean_coef2[t] * x_t ) return posterior_mean The various constants used in the above equations form a part of what’s known as a scheduler, the details of which are presently skipped here. An excellent resource for this is [11].\nFinally, having obtained a prediction for the reconstructed image, the Stable Diffusion model is trained to minimize the mean squared error between the original image and the predicted image. This is the “simple” version of the model; other versions of the model involve other objectives as well, such as an objective to learn the variances in the reverse diffusion process as well.\nSampling The reverse denoising described above can be very slow if done iteratively for each time step. So, different methods have been proposed to speed up the reverse diffusion process. Each method builds on its interpretation of the reverse diffusion process and has its pros and cons. Let’s look at some of these in detail:\nDenoising Diffusion Implicit Models (DDIM) [9]: This method uses a deterministic process (corresponding to 0 variance) for modeling reverse diffusion using strided time steps. This deterministic process can potentially map noisy samples back to the original image in the limit of a large number of steps. If we allow a learnable non-zero variance then this method is known as Denoising Diffusion Probabilistic Model (DDPM) [3].\nPseudo Linear Multi-Step Method (PLMS) [5]: This method views the reverse diffusion process as a differential equation and uses pseudo-numerical methods to solve the differential equation. They modify classical numerical methods by using a gradient part and non-linear transfer part while structuring the differential equation. Finally, they argue that DDIM is a special case of PLMS sampling and compare the performances of the two methods.\nFigure 2. Image generation results measured in terms of FID and time per step. S-PNDM uses gradients from two time-steps whereas F-PNDM uses from four. Table from [3].\nWe can see that PLMS has a higher per-step execution time than DDIM. However, the FID from PLMS is much lower than DDIM for the same number of steps.\nReferences [1] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis, 2020.\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\n[3] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.\n[4] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks, 2016.\n[5] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds, 2022.\n[6] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.\n[7] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021.\n[8] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015.\n[9] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2020.\n[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017.\n",
  "wordCount" : "2086",
  "inLanguage": "en",
  "datePublished": "2022-10-01T20:53:39-04:00",
  "dateModified": "2022-10-01T20:53:39-04:00",
  "author":{
    "@type": "Person",
    "name": "Nihal Jain"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://nihaljn.github.io/posts/understanding-stable-diffusion/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Nihal Jain",
    "logo": {
      "@type": "ImageObject",
      "url": "https://nihaljn.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://nihaljn.github.io/" accesskey="h" title="Nihal Jain (Alt + H)">Nihal Jain</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Understanding Stable Diffusion
    </h1>
    <div class="post-description">
      A deep dive into the method and code of Stable Diffusion.
    </div>
    <div class="post-meta"><span title='2022-10-01 20:53:39 -0400 EDT'>October 1, 2022</span>&nbsp;·&nbsp;Nihal Jain

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#architecture-details" aria-label="Architecture Details">Architecture Details</a><ul>
                        
                <li>
                    <a href="#autoencoder" aria-label="Autoencoder">Autoencoder</a></li>
                <li>
                    <a href="#diffusion-model" aria-label="Diffusion Model">Diffusion Model</a></li>
                <li>
                    <a href="#conditioning-model" aria-label="Conditioning Model">Conditioning Model</a></li></ul>
                </li>
                <li>
                    <a href="#training-details" aria-label="Training Details">Training Details</a><ul>
                        
                <li>
                    <a href="#autoencoder-1" aria-label="Autoencoder">Autoencoder</a></li>
                <li>
                    <a href="#diffusion-model-1" aria-label="Diffusion Model">Diffusion Model</a></li></ul>
                </li>
                <li>
                    <a href="#sampling" aria-label="Sampling">Sampling</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>This article serves to explain the Stable Diffusion [7] model and some of its implementation details. <em>Note: The Stable Diffusion model consists of several blocks carefully engineered together in a large code-base. Towards maintaining ease of simplicity, this article skips over unnecessary lines of code or modifies the code in some way without altering its logic.</em></p>
<h2 id="architecture-details">Architecture Details<a hidden class="anchor" aria-hidden="true" href="#architecture-details">#</a></h2>
<figure class="align-center ">
    <img loading="lazy" src="images/model_arch.png#center"
         alt="Figure 1. The Stable Diffusion model consists of multiple stages across several blocks as shown here. The figure shows one possible data flow path labelled in roman numerals $ I - IV $ in that order; this sequence also resembles the training of the model. The latent representation of an encoded image is gradually randomized by injecting noise in a Markov chain. This is followed by predicting the noise (and the denoised latent representation) conditioned on some information about what has to be recovered &amp;ndash; in this case, a text description of the original image. Finally, the predicted denoised latent representation can be decoded to reconstruct the original image."/> <figcaption>
            <p>Figure 1. The Stable Diffusion model consists of multiple stages across several blocks as shown here. The figure shows one possible data flow path labelled in roman numerals $ I - IV $ in that order; this sequence also resembles the training of the model. The latent representation of an encoded image is gradually randomized by injecting noise in a Markov chain. This is followed by predicting the noise (and the denoised latent representation) conditioned on some information about what has to be recovered &ndash; in this case, a text description of the original image. Finally, the predicted denoised latent representation can be decoded to reconstruct the original image.</p>
        </figcaption>
</figure>

<p>As shown in Figure 1, there are several pieces to the Stable Diffusion model. In the following sections, we will look into each block in more detail alongside the code snippets that make these blocks work.</p>
<h3 id="autoencoder">Autoencoder<a hidden class="anchor" aria-hidden="true" href="#autoencoder">#</a></h3>
<p>The autoencoder used in Stable Diffusion is similar to that in the VQGAN paper [1]. It serves the purpose of perceptual compression, and is trained using a perceptual loss and a patch-based adversarial objective. The authors argue in the paper that together, these objectives are responsible for learning semantic variation and instilling realism in the generations. Additionally, there exist two options for a regularization objective to ensure that the learned latent space distribution is zero-centered and has low variance. All objectives used for training the autoencoder are explained in more detail in section: [training details &gt; autoencoder].</p>
<p>If the regularization objective used is KL divergence between the predicted distribution and $\mathcal{N}(0, 1)$, we&rsquo;d be training the <code>AutoencoderKL</code> which we will understand in detail. <code>AutoencoderKL</code> is defined as follows:</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;Source: ldm/models/autoencoder.py&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AutoencoderKL</span>(pl<span style="color:#f92672">.</span>LightningModule):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, modelconfig, lossconfig, embed_dim):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>embed_dim <span style="color:#f92672">=</span> embed_dim
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>encoder <span style="color:#f92672">=</span> Encoder(<span style="color:#f92672">**</span>modelconfig)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>decoder <span style="color:#f92672">=</span> Decoder(<span style="color:#f92672">**</span>modelconfig)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss <span style="color:#f92672">=</span> LPIPSWithDiscriminator(lossconfig)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>quant_conv <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>modelconfig[<span style="color:#e6db74">&#34;z_channels&#34;</span>], <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>embed_dim, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>post_quant_conv <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv2d(embed_dim, modelconfig[<span style="color:#e6db74">&#34;z_channels&#34;</span>], <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># skipped irrelevant lines</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">encode</span>(self, x):
</span></span><span style="display:flex;"><span>        h <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encoder(x)
</span></span><span style="display:flex;"><span>        moments <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>quant_conv(h)
</span></span><span style="display:flex;"><span>        posterior <span style="color:#f92672">=</span> DiagonalGaussianDistribution(moments)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> posterior
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">decode</span>(self, z):
</span></span><span style="display:flex;"><span>        z <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>post_quant_conv(z)
</span></span><span style="display:flex;"><span>        dec <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decoder(z)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> dec
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input, sample_posterior<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>        posterior <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encode(input)
</span></span><span style="display:flex;"><span>        z <span style="color:#f92672">=</span> posterior<span style="color:#f92672">.</span>sample()
</span></span><span style="display:flex;"><span>        dec <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decode(z)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> dec, posterior
</span></span></code></pre></td></tr></table>
</div>
</div><p>The encoding phase of the autoencoder outputs a <code>posterior</code> over the latent space given an input image <code>x</code>. The <code>posterior</code> is assumed to have a diagonal covarinace matrix, so the posterior can be identified using <code>2*embed_dim</code> parameters (the means and variances of all dimensions). Then, a sample <code>z</code> from the <code>posterior</code> is decoded to obtain a prediction of the original image in the same space as <code>x</code>. This model is optimized using <code>LPIPSWithDiscriminator</code> which includes a perceptual loss, an adversarial loss and a regularizing KL divergence loss; we shall return to this later.</p>
<p>Further, we note here that the same <code>z</code> undergoes the diffusion process to obtain a noisy <code>z</code> so that we can train a denoising diffusion probabilistic model to estimate the denoised version given some conditioning.</p>
<p>The <code>Encoder</code> and <code>Decoder</code> defined above comprise well-known modules in deep learning such as residual blocks [2] and self-attention [10] in an intuitive architecture. So, we skip a detailed discussion of their implementations here which can be found at <code>ldm/modules/diffusionmodules/model.py</code>.</p>
<h3 id="diffusion-model">Diffusion Model<a hidden class="anchor" aria-hidden="true" href="#diffusion-model">#</a></h3>
<p>The diffusion model in Stable Diffusion has a U-Net architecture [8] with support for processing timestep embeddings, and cross-attention between the context embedding and the latent representation to be denoised.</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;Source: ldm/modules/diffusionmodules/openaimodel.py&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">UNetModel</span>(torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        time_embed_dim <span style="color:#f92672">=</span> model_channels <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>time_embed <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            linear(model_channels, time_embed_dim),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>SiLU(),
</span></span><span style="display:flex;"><span>            linear(time_embed_dim, time_embed_dim),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>input_blocks <span style="color:#f92672">=</span> [torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv2d(in_channels, model_channels, <span style="color:#ae81ff">3</span>)]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> level, mult <span style="color:#f92672">in</span> enumerate(channel_mult):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(num_res_blocks):
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>input_blocks <span style="color:#f92672">+=</span> [
</span></span><span style="display:flex;"><span>                    ResBlock(),                 <span style="color:#75715e"># processes z and timestep embed</span>
</span></span><span style="display:flex;"><span>                    AttentionBlock(),           <span style="color:#75715e"># cross-attn between z and context embed</span>
</span></span><span style="display:flex;"><span>                ] <span style="color:#75715e"># skipped arguments to modules</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>input_blocks <span style="color:#f92672">+=</span> [ResBlock()]   <span style="color:#75715e"># processes z and timestep embed</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>input_blocks <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>self<span style="color:#f92672">.</span>input_blocks)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>middle_block <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>[
</span></span><span style="display:flex;"><span>            ResBlock(),                         <span style="color:#75715e"># processes z and timestep embed</span>
</span></span><span style="display:flex;"><span>            AttentionBlock(),                   <span style="color:#75715e"># cross-attn between z and context embed</span>
</span></span><span style="display:flex;"><span>            ResBlock()                          <span style="color:#75715e"># processes z and timestep embed</span>
</span></span><span style="display:flex;"><span>        ]) <span style="color:#75715e"># skipped arguments to modules</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>output_blocks <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> level, mult <span style="color:#f92672">in</span> enumerate(channel_mult):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(num_res_blocks):
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>output_blocks <span style="color:#f92672">+=</span> [
</span></span><span style="display:flex;"><span>                    ResBlock(),                 <span style="color:#75715e"># processes z, skip conn from inp blocks, timestep embed</span>
</span></span><span style="display:flex;"><span>                    AttentionBlock()            <span style="color:#75715e"># cross-attn between z and context embed</span>
</span></span><span style="display:flex;"><span>                ] <span style="color:#75715e"># skipped arguments to modules</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>input_blocks <span style="color:#f92672">+=</span> [ResBlock()]   <span style="color:#75715e"># processes z and timestep embed</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>output_blocks <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>self<span style="color:#f92672">.</span>output_blocks)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>out <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            normalization(ch),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>SiLU(),
</span></span><span style="display:flex;"><span>            torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv2d(model_channels, out_channels, <span style="color:#ae81ff">3</span>),
</span></span><span style="display:flex;"><span>        )
</span></span></code></pre></td></tr></table>
</div>
</div><p>Similar to the autoencoder, the U-Net model comprises some well-known techniques. First, we see an MLP head to project timestep information to a high-dimensional embedding space. The input, middle and output blocks are composed of residual and cross-attention blocks which enables the generative model to lern a conditional distribution for the reverse diffusion process. Finally, the output blocks also involve skip connections from the input blocks, a feature typical to U-Net style architectures.</p>
<h3 id="conditioning-model">Conditioning Model<a hidden class="anchor" aria-hidden="true" href="#conditioning-model">#</a></h3>
<p>The Stable Diffusion models are paramaterized to be able to perform both conditional and non-conditional image generation. Towards conditional image generation, we would require a mechanism to encode the human-parsable conditioning such as text or image to be fed into the cross-attention layers of the U-Net above. So, the authors have designed domain-specific conditioning models for the various data domains that they train their models on:</p>
<ul>
<li>
<p><strong>Text-to-image generation</strong>: At the time of writing of this article, Stable Diffusion&rsquo;s text-to-image generation uses text embeddings from a frozen CLIP [6] model to obtain conditioning embeddings.</p>
</li>
<li>
<p><strong>Class-conditional generation</strong>: In order to condition on class-information, for example, classes of ImageNet, they use a single embedding layer to transform a class index to its corresponding representation.</p>
</li>
</ul>
<h2 id="training-details">Training Details<a hidden class="anchor" aria-hidden="true" href="#training-details">#</a></h2>
<p>The Stable Diffusion model is trained in two stages: (1) training the autoencoder alone, i.e., $I, IV$ only in figure 1, and (2) training the diffusion model alone after fixing the autoencoder, i.e., $I - IV$ in figure 1 but keeping $I, IV$ frozen. Let&rsquo;s look at each phase in more detail.</p>
<h3 id="autoencoder-1">Autoencoder<a hidden class="anchor" aria-hidden="true" href="#autoencoder-1">#</a></h3>
<p>The autoencoder is trained using the <code>LPIPSWithDiscriminator</code>. This loss has the following components:</p>
<ul>
<li>
<p><strong>Reconstruction Loss</strong> ($\mathcal{L}_{r}$): The mean-squared-error between the reconstructions and the predicted images in the pixel space</p>
</li>
<li>
<p><strong>Perceptual Loss</strong> ($\mathcal{L}_{p}$): The Learned Perceptual Image Patch Similarity (LPIPS) metric loss. In a nutshell, this objective tries to minimize the perceptual differences between the reconstructed and original images by making sure that the VGG features of both the images are close to each other. This is based on the empirical finding that differences in the representation spaces of computer vision models such as VGG capture perceptual distance very well compared to differences in image space or using metrics like PSNR [12].</p>
</li>
<li>
<p><strong>KL divergence Loss</strong> ($\mathcal{L}_{k}$): This is a regularization term added to the objective of the autoencoder to ensure that the latent dimension of the autoencoder is zero-centered and has low variance. Towards this the KL divergence is measure with respect to $\mathcal{N}(0, 1)$.</p>
</li>
<li>
<p><strong>Adversarial Loss</strong> ($\mathcal{L}_{a}$): The autoencoder is also trained against a Patch-GAN [patchgan] discriminator, i.e., the autoencoder is trained to maximize the scores of a discriminator which looks at various patches of any image to make a collective decision about whether the image is real or fake.</p>
</li>
</ul>
<p>The adversarial loss is scaled with an adaptive weight such that there is a balance between the adversarial and reconstruction objectives [1]:</p>
<p>$$ \lambda_{adv} = \frac{\nabla \mathcal{L}_r + \mathcal{L}_p}{\nabla \mathcal{L}_a + \delta} $$</p>
<p>Intuitively, if the gradients of $\mathcal{L}_a$ are small, then we should scale up the loss appropriately to allow faster training.</p>
<h3 id="diffusion-model-1">Diffusion Model<a hidden class="anchor" aria-hidden="true" href="#diffusion-model-1">#</a></h3>
<p>Having trained the autoencoder, we can train the diffusion model to denoise noisy latent representations from the autoencoder. This involves gradually injecting noise into the outputs of the encoder, and training a diffusion model to learn the reverse process of this noise injection.</p>
<p>The noise is injected into the latent representation in a Markov chain of $T$ time steps. Given a representation at some time step $t$, we can obtain the next time step noisy representation as:</p>
<p>$$ q(x_{t + 1} | x_t) = \mathcal{N}(\sqrt{1 - \beta_t}x_{t - 1}, \beta_t \mathbf{I}) $$</p>
<p>It can be shown that we can skip over time steps instead of iterating over each time step as:</p>
<p>$$ q(x_{t} | x_0) = \mathcal{N}(\sqrt{\bar{\alpha_t}}x_0, (1 - \bar{\alpha_t})\mathbf{I}) $$</p>
<p>This is performed in code as:</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;Source: ldm/models/diffusion/ddpm.py&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">q_sample</span>(self, x_start, t):
</span></span><span style="display:flex;"><span>    noise <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn_like(x_start)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> (
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>sqrt_alphas_cumprod[t] <span style="color:#f92672">*</span> x_start <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>sqrt_one_minus_alphas_cumprod[t] <span style="color:#f92672">*</span> noise
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></td></tr></table>
</div>
</div><p>The reverse diffusion process can be shown to be:</p>
<p>$$ q(x_{t - 1} | x_t, x_0) = \mathcal{N}(\tilde{\mu}(x_t, x_0), \tilde{\beta}_t) $$</p>
<p>This is performed in code as:</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;Source: ldm/models/diffusion/ddpm.py&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">q_posterior</span>(self, x_start, x_t, t):
</span></span><span style="display:flex;"><span>    posterior_mean <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>posterior_mean_coef1[t] <span style="color:#f92672">*</span> x_start <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>posterior_mean_coef2[t] <span style="color:#f92672">*</span> x_t
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> posterior_mean
</span></span></code></pre></td></tr></table>
</div>
</div><p>The various constants used in the above equations form a part of what&rsquo;s known as a scheduler, the details of which are presently skipped here. An excellent resource for this is [11].</p>
<p>Finally, having obtained a prediction for the reconstructed image, the Stable Diffusion model is trained to minimize the mean squared error between the original image and the predicted image. This is the &ldquo;simple&rdquo; version of the model; other versions of the model involve other objectives as well, such as an objective to learn the variances in the reverse diffusion process as well.</p>
<h2 id="sampling">Sampling<a hidden class="anchor" aria-hidden="true" href="#sampling">#</a></h2>
<p>The reverse denoising described above can be very slow if done iteratively for each time step. So, different methods have been proposed to speed up the reverse diffusion process. Each method builds on its interpretation of the reverse diffusion process and has its pros and cons. Let&rsquo;s look at some of these in detail:</p>
<ul>
<li>
<p><strong>Denoising Diffusion Implicit Models (DDIM) [9]</strong>: This method uses a deterministic process (corresponding to 0 variance) for modeling reverse diffusion using strided time steps. This deterministic process can potentially map noisy samples back to the original image in the limit of a large number of steps. If we allow a learnable non-zero variance then this method is known as Denoising Diffusion Probabilistic Model (DDPM) [3].</p>
</li>
<li>
<p><strong>Pseudo Linear Multi-Step Method (PLMS) [5]</strong>: This method views the reverse diffusion process as a differential equation and uses pseudo-numerical methods to solve the differential equation. They modify classical numerical methods by using a gradient part and non-linear transfer part while structuring the differential equation. Finally, they argue that DDIM is a special case of PLMS sampling and compare the performances of the two methods.</p>
</li>
</ul>
<figure class="align-center ">
    <img loading="lazy" src="images/ddim_plms.png#center"
         alt="Figure 2. Image generation results measured in terms of FID and time per step. S-PNDM uses gradients from two time-steps whereas F-PNDM uses from four. Table from [3]."/> <figcaption>
            <p>Figure 2. Image generation results measured in terms of FID and time per step. S-PNDM uses gradients from two time-steps whereas F-PNDM uses from four. Table from [3].</p>
        </figcaption>
</figure>

<p>We can see that PLMS has a higher per-step execution time than DDIM. However, the FID from PLMS is much lower than DDIM for the same number of steps.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis, 2020.</p>
<p>[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.</p>
<p>[3] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.</p>
<p>[4] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks, 2016.</p>
<p>[5] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds, 2022.</p>
<p>[6] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.</p>
<p>[7] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021.</p>
<p>[8] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015.</p>
<p>[9] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2020.</p>
<p>[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://nihaljn.github.io/">Nihal Jain</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
