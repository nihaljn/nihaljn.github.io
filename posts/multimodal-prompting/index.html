<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Multimodal Prompting for Image Generation | Nihal Jain</title>
<meta name="keywords" content="">
<meta name="description" content="Enabling the use of multiple modalities while prompting Stable Diffusion.">
<meta name="author" content="Nihal Jain">
<link rel="canonical" href="https://nihaljn.github.io/posts/multimodal-prompting/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bf6adf603ab99a8445bb5935636735be50c0bc4c2ac5f4e240c334a755c1b543.css" integrity="sha256-v2rfYDq5moRFu1k1Y2c1vlDAvEwqxfTiQMM0p1XBtUM=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://nihaljn.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://nihaljn.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://nihaljn.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://nihaljn.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://nihaljn.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>




<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script><meta property="og:title" content="Multimodal Prompting for Image Generation" />
<meta property="og:description" content="Enabling the use of multiple modalities while prompting Stable Diffusion." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://nihaljn.github.io/posts/multimodal-prompting/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-01T20:56:16-04:00" />
<meta property="article:modified_time" content="2022-10-01T20:56:16-04:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Multimodal Prompting for Image Generation"/>
<meta name="twitter:description" content="Enabling the use of multiple modalities while prompting Stable Diffusion."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://nihaljn.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Multimodal Prompting for Image Generation",
      "item": "https://nihaljn.github.io/posts/multimodal-prompting/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Multimodal Prompting for Image Generation",
  "name": "Multimodal Prompting for Image Generation",
  "description": "Enabling the use of multiple modalities while prompting Stable Diffusion.",
  "keywords": [
    
  ],
  "articleBody": " In the past few months, we have experienced groundbreaking progress on the front of image generation models. While some of these models have been available behind black-box user interfaces (or only for fixed prompts) [dalle, parti, make-a-scene], some of them [stable diffusion] have been open-sourced with practically no restrictions! This has led to a tremendous surge in the use, analysis and extension of models like Stable Diffusion which is now being used for very creative applications such as infinite image outpainting, etc. [textual inversion, outpainting].\nThis article presents yet another extension to the Stable Diffusion for image generation: a simple and fast strategy towards prompting Stable Diffusion with an arbitrary composition of image and text tokens. This is useful for the following reasons:\nAchieving high quality, desirable outputs from image generation models requires rigorous prompt tuning. While the community has reverse-engineered several interesting tricks for working with image generation models, this leads to a bad user-experience. In zero-shot settings, when either the user or the model comes across a concept that is not within their vocabulary or too cumbersome to describe, the model may be better suited with processing the prompt if it can be “shown” the new concept. Figure 1 shows an example of our method in action and more examples are shown below.\nMethod Enabling multimodal prompting requires the underlying model to have a joint understanding of the different modalities. The straightforward solution for this would be to train or finetune Stable Diffusion so that its conditioning model can input multiple modalities, and the outputs of which can be continued to use as before. Another method for this could be to use textual inversion [textual inversion]: we can finetune the model to represent each of the non-text concepts in the prompt as textual tokens, and use the newly learned tokens to replace the images in the prompts. However, in this article, we will look at a simpler strategy that requires no training in any way, with negligible additional overhead in comparison to the original Stable Diffusion.\nThe high level overview of our method is:\nMaintain a rich corpus of prompts (or sub-prompts) that can textually explain various aspects of the images in the prompt in detail\nFor each image in the prompt, project it into the pooler representation of CLIP and find the text nearest neighbors from above to the image in this space\nA meaningful prompt is obtained by concatenating a diverse set of nearest neighbors. This is achieved by using Maximum Marginal Relevance (MMR) [] over the nearest neighbors using CLIP for similarity calculations in the algorithm\nReplace each image in the prompt with its “translated” text version to obtain a text-only prompt which can be used in Stable Diffusion as originally intended\nLet us look at each piece of this method in more detail.\nData: Curating an index of text prompts Our method requires being able to retrieve text prompts that can adequately describe some image used in a prompt. We use Simulacra Aesthetic Captions [dataset] which is a curated dataset of over 40K prompts written by humans.\nMost prompts in this datset are long-winded and highly detailed such that they only describe the respective images intended to be produced. This set of prompts may not be able to describe a new image that has details different from any image this dataset describes. So, we chunk each prompt in the dataset into smaller n-grams (in our experiments, we fix $n = 4$) such that each n-gram can be expected to describe only an atomic detail of some image. Thus, in expectation, combining different ngrams across the dataset should expand the scope of images describable by the dataset. While blind composition of random n-grams from the dataset may not lead to prompts that are intelligible or even grammarly correct, we can expect our retrieval strategy to obtain a diverse set of terms in the composition such that each provides meaningful signal to Stable Diffusion about what has to be generated.\nFor example, the prompt 'A crowded subway car, the fluorescent lights flickering and casting an eerie glow on the people inside' will be mapped to ['A crowded subway car', 'crowded subway car, the', 'subway car, the fluorescent', ..., 'on the people inside'].\nMapping images to text Given any image, we are interested in retrieving a set of n-grams such that composing them adequately describes the image. We utilize CLIP’s pooled representation space to perform cross-modal nearest-neighbor search where the query is the image to be descibed and the index comprises all the n-grams.\nSimply picking the top-$k$ neighbors from a nearest-neighbor search might lead to uninteresting results. For example, if the top-$k$ neighbors are all very close to each other, the resulting combination of them will not sufficiently describe the image we’re interested in. Towards mitigating this, we use a popular re-ranking algorithm, Maximum Marginal Relevance (MMR) to pick a diverse set of n-grams from the top-$k$ neighbors which can together be used to describe the image better. In our experiments, we set $k=100$.\nFigure 2 below shows a schematic of this approach.\nGenerating prompts We can compose the top-$m$ n-grams from the above step by simply concatenating them. More sophistacted approaches for combining these which retain grammatic correctness and better capture context of the remaining prompt are left as future work.\nThe specific value of $m$ can be used to weight different images in the prompt. $m = 4$ was found to be an average value and assigning weights to images relative to this value would change their relative importance for generation.\nFor example, the resulting prompt from the input image in figure 2 would be: tropical beach covered in water unsplash 4k photograph pastel palette matte painting pink at sunset.\nResults We can perform the above procedure for each image in the prompt and convert our multimodal prompt to a text-only prompt. This can then be used in the Stable Diffusion model as originally intended.\nLet’s look at some results.\nReferences ",
  "wordCount" : "997",
  "inLanguage": "en",
  "datePublished": "2022-10-01T20:56:16-04:00",
  "dateModified": "2022-10-01T20:56:16-04:00",
  "author":{
    "@type": "Person",
    "name": "Nihal Jain"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://nihaljn.github.io/posts/multimodal-prompting/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Nihal Jain",
    "logo": {
      "@type": "ImageObject",
      "url": "https://nihaljn.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://nihaljn.github.io/" accesskey="h" title="Nihal Jain (Alt + H)">Nihal Jain</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Multimodal Prompting for Image Generation
    </h1>
    <div class="post-description">
      Enabling the use of multiple modalities while prompting Stable Diffusion.
    </div>
    <div class="post-meta"><span title='2022-10-01 20:56:16 -0400 EDT'>October 1, 2022</span>&nbsp;·&nbsp;Nihal Jain

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#method" aria-label="Method">Method</a><ul>
                        
                <li>
                    <a href="#data-curating-an-index-of-text-prompts" aria-label="Data: Curating an index of text prompts">Data: Curating an index of text prompts</a></li>
                <li>
                    <a href="#mapping-images-to-text" aria-label="Mapping images to text">Mapping images to text</a></li>
                <li>
                    <a href="#generating-prompts" aria-label="Generating prompts">Generating prompts</a></li></ul>
                </li>
                <li>
                    <a href="#results" aria-label="Results">Results</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><figure>
    <img loading="lazy" src="images/einstein.png"/> 
</figure>

<p>In the past few months, we have experienced groundbreaking progress on the front of image generation models. While some of these models have been available behind black-box user interfaces (or only for fixed prompts) [dalle, parti, make-a-scene], some of them [stable diffusion] have been open-sourced with practically no restrictions! This has led to a tremendous surge in the use, analysis and extension of models like Stable Diffusion  which is now being used for very creative applications such as infinite image outpainting, etc. [textual inversion, outpainting].</p>
<p>This article presents yet another extension to the Stable Diffusion for image generation: a simple and fast strategy towards prompting Stable Diffusion with an arbitrary composition of image and text tokens. This is useful for the following reasons:</p>
<ul>
<li>Achieving high quality, desirable outputs from image generation models requires rigorous prompt tuning. While the community has reverse-engineered several interesting tricks for working with image generation models, this leads to a bad user-experience.</li>
<li>In zero-shot settings, when either the user or the model comes across a concept that is not within their vocabulary or too cumbersome to describe, the model may be better suited with processing the prompt if it can be &ldquo;shown&rdquo; the new concept.</li>
</ul>
<p>Figure 1 shows an example of our method in action and more examples are shown below.</p>
<h2 id="method">Method<a hidden class="anchor" aria-hidden="true" href="#method">#</a></h2>
<p>Enabling multimodal prompting requires the underlying model to have a joint understanding of the different modalities. The straightforward solution for this would be to train or finetune Stable Diffusion so that its conditioning model can input multiple modalities, and the outputs of which can be continued to use as before. Another method for this could be to use textual inversion [textual inversion]: we can finetune the model to represent each of the non-text concepts in the prompt as textual tokens, and use the newly learned tokens to replace the images in the prompts. However, in this article, we will look at a simpler strategy that requires no training in any way, with negligible additional overhead in comparison to the original Stable Diffusion.</p>
<p>The high level overview of our method is:</p>
<ul>
<li>
<p>Maintain a rich corpus of prompts (or sub-prompts) that can textually explain various aspects of the images in the prompt in detail</p>
</li>
<li>
<p>For each image in the prompt, project it into the pooler representation of CLIP and find the text nearest neighbors from above to the image in this space</p>
</li>
<li>
<p>A meaningful prompt is obtained by concatenating a diverse set of nearest neighbors. This is achieved by using Maximum Marginal Relevance (MMR) [] over the nearest neighbors using CLIP for similarity calculations in the algorithm</p>
</li>
<li>
<p>Replace each image in the prompt with its &ldquo;translated&rdquo; text version to obtain a text-only prompt which can be used in Stable Diffusion as originally intended</p>
</li>
</ul>
<p>Let us look at each piece of this method in more detail.</p>
<h3 id="data-curating-an-index-of-text-prompts">Data: Curating an index of text prompts<a hidden class="anchor" aria-hidden="true" href="#data-curating-an-index-of-text-prompts">#</a></h3>
<p>Our method requires being able to retrieve text prompts that can adequately describe some image used in a prompt. We use Simulacra Aesthetic Captions [dataset] which is a curated dataset of over 40K prompts written by humans.</p>
<p>Most prompts in this datset are long-winded and highly detailed such that they only describe the respective images intended to be produced. This set of prompts may not be able to describe a new image that has details different from any image this dataset describes. So, we chunk each prompt in the dataset into smaller n-grams (in our experiments, we fix $n = 4$) such that each n-gram can be expected to describe only an atomic detail of some image. Thus, in expectation, combining different ngrams across the dataset should expand the scope of images describable by the dataset. While blind composition of random n-grams from the dataset may not lead to prompts that are intelligible or even grammarly correct, we can expect our retrieval strategy to obtain a diverse set of terms in the composition such that each provides meaningful signal to Stable Diffusion about what has to be generated.</p>
<p>For example, the prompt <code>'A crowded subway car, the fluorescent lights flickering and casting an eerie glow on the people inside'</code> will be mapped to <code>['A crowded subway car', 'crowded subway car, the', 'subway car, the fluorescent', ..., 'on the people inside']</code>.</p>
<h3 id="mapping-images-to-text">Mapping images to text<a hidden class="anchor" aria-hidden="true" href="#mapping-images-to-text">#</a></h3>
<p>Given any image, we are interested in retrieving a set of n-grams such that composing them adequately describes the image. We utilize CLIP&rsquo;s pooled representation space to perform cross-modal nearest-neighbor search where the query is the image to be descibed and the index comprises all the n-grams.</p>
<p>Simply picking the top-$k$ neighbors from a nearest-neighbor search might lead to uninteresting results. For example, if the top-$k$ neighbors are all very close to each other, the resulting combination of them will not sufficiently describe the image we&rsquo;re interested in. Towards mitigating this, we use a popular re-ranking algorithm, Maximum Marginal Relevance (MMR) to pick a diverse set of n-grams from the top-$k$ neighbors which can together be used to describe the image better. In our experiments, we set $k=100$.</p>
<p>Figure 2 below shows a schematic of this approach.</p>
<figure>
    <img loading="lazy" src="images/search_schematic.png"/> 
</figure>

<h3 id="generating-prompts">Generating prompts<a hidden class="anchor" aria-hidden="true" href="#generating-prompts">#</a></h3>
<p>We can compose the top-$m$ n-grams from the above step by simply concatenating them. More sophistacted approaches for combining these which retain grammatic correctness and better capture context of the remaining prompt are left as future work.</p>
<p>The specific value of $m$ can be used to weight different images in the prompt. $m = 4$ was found to be an average value and assigning weights to images relative to this value would change their relative importance for generation.</p>
<p>For example, the resulting prompt from the input image in figure 2 would be: <code>tropical beach covered in water unsplash 4k photograph pastel palette matte painting pink at sunset</code>.</p>
<h2 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h2>
<p>We can perform the above procedure for each image in the prompt and convert our multimodal prompt to a text-only prompt. This can then be used in the Stable Diffusion model as originally intended.</p>
<p>Let&rsquo;s look at some results.</p>
<figure>
    <img loading="lazy" src="images/tiger.png"/> 
</figure>

<figure>
    <img loading="lazy" src="images/cyber1.png"/> 
</figure>

<figure>
    <img loading="lazy" src="images/cyber2.png"/> 
</figure>

<figure>
    <img loading="lazy" src="images/cyber3.png"/> 
</figure>

<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://nihaljn.github.io/">Nihal Jain</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
